* DONE MAKE CHEATSHEET
  CLOSED: [2019-02-27 Wed 12:31] DEADLINE: <2019-02-15 Fri>

  - State "DONE"       from "TODO"       [2019-02-27 Wed 12:31]
* Assignments
** DONE HW1
   CLOSED: [2019-01-22 Tue 00:32] DEADLINE: <2019-01-25 Fri>
   - State "DONE"       from "TODO"       [2019-01-22 Tue 00:32]
** DONE HW2
   CLOSED: [2019-02-01 Fri 11:32] DEADLINE: <2019-02-01 Fri>
   - State "DONE"       from "TODO"       [2019-02-01 Fri 11:32]
** DONE HW3
   CLOSED: [2019-02-11 Mon 13:15] DEADLINE: <2019-02-08 Fri>
** DONE HW4
   CLOSED: [2019-02-15 Fri 12:39] DEADLINE: <2019-02-15 Fri>
   - State "DONE"       from "TODO"       [2019-02-15 Fri 12:39]
** TODO HW5
   DEADLINE: <2019-02-22 Fri>
** TODO HW6
   DEADLINE: <2019-03-01 Fri>
** TODO HW7
   DEADLINE: <2019-03-08 Fri>
** TODO HW8
   DEADLINE: <2019-03-15 Fri>
** TODO HW9
   DEADLINE: <2019-03-22 Fri>
** TODO HW10
   DEADLINE: <2019-03-29 Fri>
** TODO HW11
   DEADLINE: <2019-04-05 Fri>
** TODO HW12
   DEADLINE: <2019-04-12 Fri>
** TODO HW13
   DEADLINE: <2019-04-19 Fri>
** TODO HW14
   DEADLINE: <2019-04-26 Fri>
** TODO HW15
   DEADLINE: <2019-05-03 Fri>
* Exams
** DONE Exam 1 (in class)
   CLOSED: [2019-02-27 Wed 12:31] DEADLINE: <2019-02-18 Mon>
   - State "DONE"       from "TODO"       [2019-02-27 Wed 12:31]
** TODO Exam 2 (in class)
   DEADLINE: <2019-04-01 Mon>
* Notes
** Chapter 2 :: Probability
*** Probability Rules
    - $P(S)=1;\ P(\varnothing)=0$
    - For any event A $0\leq P(A) \leq 1$
    - $P(A^c)=1-P(A)$ (Complement Rule)
    - Addition Rule for mutually exclusive events :: If A and B are disjoint then $P(A\cup B)=P(A)+P(B)$
    - General Addition Rule :: $P(A\cup B)=P(A)+P(B)-P(A\cap B)$
    - Product Rule for Independent Events :: If A and B are independent then $P(A\cap B)=P(A)P(B)$
    - Conditional Probability :: The probability that event A occurs given event B has occured \[P(A|B)=\frac{P(A\cap B)}{P(B)}\], provided $P(B)\neq0$
    - General Product Rule :: For any events A and B, $P(A\cap B)=P(A)P(B|A)=P(B)P(A|B)$
*** Definitions/Theorems
    - Random Experiment :: can result in one of several random outcomes
    - Sample Space :: set of all possible outcomes S
    - Discrete :: S is discrete if it contains finite or countable infinite # of outcomes
    - Continous :: S is continuous if it contains an interval
    - Event :: subset of S
               Example: roll a die - some events: roll a 2, A={2}; roll an odd, B={1,3,5}
    - Multiplication Rule :: Operation has k steps with $n_k$ ways to complete $k^th$ step, outcomes \[n_1*n_2*...*n_k\]
         Example: license plates lll nnn
         a) how many possible plates? $(26)(26)(26)(10)(10)(10)=17,576,000$
         b) how many without repeats? $(26)(25)(24)(10)(9)(8)$
         c) if first letter is A and last number is even? \[(1)(26)^2(10)^2(5)\]
         d) same as c, without repeats? $(1)(25)(24)(9)(8)(5)$
    - Theorem :: the number of permutations of N distinct elements is N!
                 Example: 26 letters => \[26! \ permutations\, = 4.03x10^{26}\]
    - Theorem :: the number of ways r elements can be selected from n distinct elements with regard to order is \[_nP_r=P^n_r=\frac{n!}{(n-r)!}\]
                 Example: 4 students {a,b,c,d}, 1 selected for president, 1 for vp \[(ab \neq  ba)\]
                 \[_4P_2 = \frac{4!}{(4-2)!} = 12\]
    - Theorem :: the number of ways r elements can be selected from n distinct elements without regard to order is \[ _nC_r=C^n_r={n \choose r}=\frac{n!}{r!(n-r)!} \]
                 Example: 4 students {a,b,c,d}, choose 2 for trip (ab=ba) -> \[\frac{4!}{2!(4-2)!} =\frac{24}{4}=6\]
                 \[{n\choose r} = \frac{_nP_r}{n!}\]
                 Example: Powerball. 69 white => select 5; 26 red -> select 1; ${26 \choose 1}{69 \choose 5} = 292,201,338$
    - Theorem :: With n objects of k types, $n_1$ of type 1 etc., the number of permutations is \[\frac{n!}{n_1!(n_2!)\ldots(n_k!)}\]
    - Probability :: the proportion of times event occurs in the long run
    - Theorem :: if S contains N equally likely outcomes, then
                 1) probability of any outcome is \[\frac{1}{N}\]
                 2) probability of event A is \[P(A) = \frac{\#\ of \ outcomes\ in \ A}{N}\]
                 Example: roll die 2 times
                 a) S = {11, 21, ... , 66} equally likely
                 b) A = product of two rolls equals six -> \[P(A)=\frac{4}{36}=\frac{1}{9}\]
                 c) B = absolute difference is two -> \[P(B) = \frac{8}{36} = \frac{2}{9}\]
    - Theorem :: suppose S is discrete. The probability of A is the sum of the probability of all outcomes in A.
                 Example: 4-pack of cans, how many dented? S = {0,1,...,4}
                 | # | Probability |
                 |---+-------------|
                 | 0 |          .8 |
                 | 1 |          .1 |
                 | 2 |         .05 |
                 | 3 |         .02 |
                 | 4 |         .03 |
                 |---+-------------|
                 A = 3 or fewer
                 P(A) = .8 + .1 + .05 + .02 = .97
    - Intersection :: "and" $\cap$  ##### add venn diagrams from notes (use scimax, inkscape, tikz, etc)
    - Union :: "or" $\cup$
    - Complement :: "Not A" $A^c = A'$
                    Example: roll n die  ##### Change to latex table or get rid of table altogether
                    | A = roll 2    | {2}                    |
                    | B = roll odd  | {1,3,5}                |
                    | C = roll even | {2,4,6}                |
                    | $A\cap C$     | {2}                    |
                    | $A\cup C$     | {2,4,6}                |
                    | $B^c$         | {2,4,6}                |
                    | $B\cap C$     | $\{\}\ =\ \varnothing$ |
                    | $B\cupC$      | S                      |
    - Mutual Exlusivity :: Two events A and B are mutually exclusive if $A\capB = \varnothing$
    - Independence :: Two events are independent if the occurence of one event does not influence the probability of the other event
                      Example: Bowl -> 3 red, 2 white chips; randomly select 2 chips with replacement
                      A = red first; B = red second -> independent because of the replacement
    - Example :: Heart attack vs. aspirin  ## there are two more if you want to add later
                 |---------+-----+-------+-------|
                 |         |   H |    H' | Total |
                 |---------+-----+-------+-------|
                 | Placebo | 189 | 10845 | 11034 |
                 | Aspirin | 104 | 10933 | 11037 |
                 | Total   | 293 | 21778 | 22071 |
                 |---------+-----+-------+-------|
                 a) \[P(H)=\frac{293}{22071}=.0133\]

                 b) \[P(H\cap A)=\frac{104}{22071}=.00471\]

                 \[P(H\cap A)=P(H)P(A|H)=\frac{293}{22071}\left(\frac{104}{293}\right)=\frac{104}{22071}\]

                 \[P(H\cap A)=P(A)P(H|A)=\frac{11037}{22071}\left(\frac{104}{11037}\right)=\frac{104}{22071}\]

                 c) \[(P(H|A)=\frac{104}{11037}=.0094\]

                 \[(P(H|A)=\frac{P(H\cap A)}{P(A)} = \frac{104}{22071}\left(\frac{22071}{11037}\right)=\frac{104}{11037}\]
    - Example :: Bowl has 2 red and 3 white chips -> randomly select 2 chips wihout replacement
                 a)\[P(R_1\cap W_2)=P(R_1)P(W_2|R_1)=\frac{2}{5}\frac{3}{4}\]
                 b)\[P(R_2|W_1)=\frac{2}{4}\]
                 c)\[P(R_1\cap W_1) = 0\]
                 d) find probability of exactly one red chip
                 \[P[(R_1\cap W_2)\cup (W_1\cap R_2)] = P(R_1\cap W_2) + P(W_1\cap R_2) = P(R_1)P(W_2|R_1) + P(W_1)P(R_2|W_1)\]
                 e) $P(R_1\cup R_2)=P(R_1)+P(R_2)-P(R_1\cap R_2)=P(R_1)+P(R_2)-P(R_1)P(R_2|R_1)$
    - Theorem :: Mathematical test for independence -> Events are A and B are independent IFF ## combine with definition
                 a) P(A|B) = P(A)
                 b) P(B|A) = P(B)
                 c) $P(A\cap B) = P(A)P(B)$
    - Theorem :: Law of Total Probability
                 Suppose events $E_1,\ldots, E_n$ form a partition, i.e.
                 a) $E_i \cap E_j = \varnothing \ \forall i\neq j$ (no overlap)
                 b) $E_1 \cup E_2 \cup \ldots\cup E_n = S$
                 For any event A, $P(A) = P\[(E_1\cap A)\cup (E_2 \cap A)\ldots\cup (E_n\cap A)\]$
    - Theorem :: Baye's Theorem
                 Suppose events $E_1,\ldots, E_n$ form a partition,
                 Then for any event A $(P(A)\neq 0)$
                 $P(E_i|A)=\frac{P(E_i\cap A)}{P(A)}$
                 \[P(E_i|A)=\frac{P(E_i)P(A|E_i)}{\sum_{j=1}^{n}P(E_j)P(A|E_j)}\]
    - Example :: Have 3 bowls
                 | Bowl | Red | White |
                 |------+-----+-------|
                 |    1 |   1 |     4 |
                 |    2 |   2 |     1 |
                 |    3 |   4 |     1 |
                 |------+-----+-------|
                 Randomly select a bowl, randomly select a chip
                 a) find probability that you select a red chip.
                 \[\begin{align}
                 P(R)&=P[(B_1\cap R)\cup (B_2\cap R)\cup (B_3\cap R)]\\
                 &=\sum_{i=1}^3 P(B_i\cap R)\\
                 &=\sum_{i=1}^3 P(B_i)P(R|B_i)\\
                 &=\frac{1}{3}(\frac{1}{5})+\frac{1}{3}(\frac{2}{3})+\frac{1}{3}(\frac{4}{5})=\frac{5}{9}
                 \end{align}\]
                 b) Given chip was red find probability that it came from b3
                 \[\begin{align}
                 P(B_3|R)&=\frac{P(B_3\cap R)}{P(R)}\\
                 &=\frac{P(B_3)P(R|B_3)}{P(R)}\\
                 &=\frac{\frac{1}{3}\frac{4}{5}}{\frac{5}{9}} = \frac{12}{25}= .48
                 \end{align}\]
                 c) Given chip was red find probability that it came from b2
                 \[\begin{align}
                 P(B_2|W)&=\frac{P(W\cap B_2}{P(W)}\\
                 &=\frac{\frac{1}{3}\frac{1}{3}}{\frac{4}{9}}=\frac{1}{4}=.25
                 \end{align}\]

                 - Example :: A Boy
                              | pocket | #blue | #white |
                              |--------+-------+--------|
                              | left   |     5 |      4 |
                              | right  |     4 |      5 |
                              |--------+-------+--------|
                              Randomly select a marble from left and transfer to right
                              Randomly select a marble from right
                              a) Find probability of $B_r$
                              \[\begin{align}
                              P(B_r)&=P(B_l\cap B_r)&+&P(W_l\cap B_r)\\
                              &=P(B_l)P(B_r|B_l)&+&P(W_l)P(B_r|W_l)\\
                              &=\frac{5}{9}\frac{5}{10}&+&\frac{4}{9}\frac{4}{10}\\
                              &=\frac{41}{90}
                              \end{align}\]
                              b) Find probability of blue drawn from left given a blue is drawn from right
                              \[\begin{align}
                              P(B_l|B_r)&=\frac{P(B_l\cap B_r)}{P(B_r)}\\
                              &=\frac{P(B_l)P(B_r|B_l)}{P(B_r)}\\
                              &=\frac{\frac{5}{9}\frac{5}{10}}{\frac{41}{90}}\\
                              &=\frac{25}{41}=0.610
                              \end{align}\]
** Chapter 3 :: Random Variables
   - Random variable :: Function that assigns a real number to each outcome in S
        *X = random variable (changes from experiment to experiment)
        *x = number ( possible value of X)
   - Discrete random variable :: Discrete random variable can assume finite or countably infinite
   - Continous random variable :: can assume any number in interval
   - Example :: flip a coin twice
                S={HH,HT,TH,TT} -> equally likely
                Let x = number heads
                a) What is the support of x?
                $S_x=\{0,1,2\}$
                b) Find probability mass function of X (Probability distribution of X)
                | x           |   0 |   1 |   2 |
                | f(x)=P(X=x) | 1/4 | 1/2 | 1/4 |
                Formulaic: $f(x)=P(X=x)={2\choose x}\frac{1}{4}$
                NOTE: suppose X is discrete random variable, then
                a) $f(x)> 0 \ \forall \ x \in S_x$
                b) \[\sum_{x\in S_x}{}f(x) = 1\]
   - Example :: Bowl has 6 chips: 2->"2", 4->"4"
                Randomly select 2 chips w/o replacement
                let X = sum of the 2 chips -> s$S_x=\{4,6,8\}$
                a)find probability mass function of x
                | x           |    4 |     6 |     8 |
                | f(x)=P(X=x) | 2/30 | 16/30 | 12/30 |
                b)
                \[\begin{align}
                P(x\geq 6|x\leq 6)&=\frac{16}{18}\\
                &=\frac{P(x\geq 6, x\leq 6)}{P(x\leq 6}\\
                &=\frac{P(x=6)}{P(x\leq 6}
                \end{align}\]   ;; "," = "and"
   - Cumulative Distribution Function :: CDF of a discrete random variable is $F(x)=P(X\leq x)=\sum_{x_i \leq x}^{}f(x) \qquad -\infty < x < \infty$
        where \[\begin{align}&1) \ \lim_{x\to -\infty}F(x)=0\\&2) \ \lim_{x\to \infty}F(x)=1\\ &3) \ if x\leq y\  then\  F(x)\leq F(y)\end{align}\]
   - Example :: flip a coin 2 times, x = # heads
                | x    |   0 |   1 |   2 |
                | f(x) | 1/4 | 1/2 | 1/4 |
                a) find CDF of x:

                \[F(x) = P(X\leq x)=\begin{cases}0\quad &x<0\\\frac{1}{4}&0\leq x <1\\\frac{3}{4}&1\leq x <2\\1 &2\leq x\end{cases}\]

                b)$P(x\leq1)=F(1)=\frac{3}{4}$
                c)$P(x>1)=1-F(1)=\frac{1}{4}$
                d)$P(x\leq 0.7)=\frac{1}{4}$
   - Example :: suppose x has pmf f(x)=x/6 for x=1,2,3
                a) find cdf of x
                \[F(x)=\begin{cases}0\quad &x<1\\\frac{1}{6}&1\leq x<2\\ \frac{1}{2}&2\leq x <3\\ 1&3\leq x\end{cases}\]
                b) $P(x<2)=F(2^{-})=\frac{1}{6}$
   - Expected Value :: if x is a discrete random variable, $\mu = E(X)=\sum_{x\in S_x}^{}xf(x)$ -> lon run average
                       if h(x) is a function of X, then $E[h(X)]=\sum_{x\in S_x}^{}h(x)f(x)$
   - Variance :: variance of X is
                 \[\begin{align}\sigma^2=Var(X)&=E[(X-\mu)^2]\\&=\sum_{x\in S_x}^{}(x-\mu)^2f(x)\end{align}\]
                 descirbes variability in units squared; represents the typical squared deviation from the mean
   - Standard Deviation :: standard deviation of X is $\sigma=SD(X)=\sqrt{Var(x)}$
        descirbes variability in units; represents the typical deviation from the mean
   - Example :: Have battery "2-pack" Let X=#low voltage
                Known the pmf of x is
                #+PLOT: ind:1 type:2d with:hist set:"xrange [0:5]" set:"yrange [0:1]"
                | x | f(x) |
                |---+------|
                | 0 |   .7 |
                | 1 |   .2 |
                | 2 |   .1 |

   NOTE: Look at org manual to plot this
   a) $\mu=E(x)=\sum_{x\in S_x}^{}xf(x)=0(.7)+1(.2)+2(.1)=0.4$
   b) \[\begin{align}\sigma^2&=var(X)=E[(X-\mu)]^2\\&=\sum_{x\in S_x}^{}(x-\mu)^2 f(x)\\&=(0-.4)^2(.7)+(1-.4)^2(.2)+(2-.4)^2(.1)\\&=.44 \end{align}\]$
   c) $\sigma=SD(X)=\sqrt{var(X)}=\sqrt{.44}=0.663$
   d) $E(X^2)=\sum_{x\in S_x}^{}x^2f(x)=0^2(.7)+1^2(.2)+2^2(.1)=0.6$
   - Theorem :: Computational Formula for the variance
                $var(X)=E(X^2)-\mu^2=E(X^2)-(E(X))^2$

                proof \[\begin{align}var(X)&=E[(x-\mu)^2\\&=\sum_{s\in S)x}^{}(x-\mu)^2f(x)\\&=\sum_{x}^{}(x^2-2\mu x+\mu^2)f(x)\\&=\sum_{x}^{}x^2f(x)-2\mu \sum_{x}^{}xf(x)+\mu^2 \sum_{x}^{}f(x)\\&=E(X^2)-\mu^2\\&=E(X^2)-(E(x))^2\end{align}\]

                e) \[\begin{align}var(X)&=E(X^2)-(E(x))^2\\&=.6-(.4)^2\\&=.44\end{align}\]
   - Binomial Distribution :: suppose an experiment consists of n independent Bernoulli trials where the probability of success in each trial is p
        NOTE: Bernoulli trial can result in "success" or "Failure"
        X = # of successes in the n trials
        then X has a binomial distribution with "parameters" n and p X~Bin(n,p) $\binom{n}{k}p^k(1-p)^{n-k}$ -- prob x has value k
   - Theorem :: X~Bin(n,p) iff
                f(x) = P(X=x) = $\binom{n}{x}p^x(1-p)^{n-x}$
                for x = 0,1,...,n
                NOTE: Binomial Expansion
                For constants a and b $(a+b)^n= \sum_{x=0}^{n}\binom{n}{x}a^xb^{n-x}$
                \[\begin{align}(a+b)^2&=\sum_{x=0}^{2}\binom{2}{x}a^xb^{2-x}\\&=\binom{2}{0}a^0b^2+\binom{2}{1}a^1b^1+\binom{2}{2}a^0b^2\\&=b^2+2ab+a^2\end{align}\]
                let a=p b=1-p
                $\sum_{x=0}^{n}\binom{n}{x} p^x(1-p)^{n-x}=(p+(1-p))^n$
                - Theorem :: If X~Bin(n,p), then
                             a) E(x)=np
                             b) \[\sigma^2=np(1-p)\]

   - Proof :: E(X)
              \[\begin{align}E(x)&=\sum_{x}^{}xf(x)\\&=\sum_{x=1}^{n}x \binom{n}{x}p^x(1-p)^{n-x}\\&=\sum_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}\\
              &=np \sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!((n-1)-(x-1))!}p^{x-1}(1-p)^{(n-1)-(x-1)}\\&=np \sum_{x=1}^{n} \binom{n-1}{x-1}p^{x-1}(1-p)^{(n-1)-(x-1)}\\
              &=np \sum_{y=0}^{m} p^y(1-p)^{m-y} \quad ; \ m=n-1, y=x-1\\&=np\end{align}\]
   - Example :: Roll die 10 times, let X = # of 6's
                Find P(X=2)
                Old way: \[P(X=2)=P(b_1b_2b_3^c\ldots b_10^c)+ \ every \ other \ permutation\]
                Bin Way: X~Bin(n=10, p = 1/6)
                \[\begin{align}P(X=2)&=\binom{n}{x}p^x(1-p)^{n-x}\\&=\binom{10}{2}(\frac{1}{6})^2(\frac{5}{6})^8\\&=0.2907\end{align}\]
   - Example :: 2% of tires are defective - Randomly Select 30 tires (independent)
                let X = # of defective tires
                X~Bin(n=30, p=.02)
                a) \[\begin{align}P(X=3)&=\binom{n}{x}p^x(1-p)^{n-x}\\&=\binom{30}{3}.02^3(1-0.02)^{30-3}\\&=0.0188\end{align}\]

                b) \[\begin{align}(\geq 1)&=(P(x=1)\ldots +P(X=30)\\&=1-P(X<1)\\&=1-P(X=0)\\&=1-\binom{n}{x}p^x(1-p)^{n-x}\\&=1-\binom{30}{0}0.02(1-.02)^30\\&=0.4545\end{align}\]
                c) on avg how may def
                $\mu=E(x)=np=30(0.02)$
                d) $SD(x)=\sqrt{.588}=0.767$
                NOTE: A Bernoulli dist is a binomial dist with n=1
                x~Bern(p)                $f(x)=P(X=x)=p^x(1-p)^{1-x}\quad x=0,1$
   - Negative binomial distribution :: Sequence of independent Bernoulli trials with probability of success p.
        X = trial on which get rth success, then    (ie trials are not fixed, go until r success)
        X~NB(r,p)
     - Theorem :: X~NB(r,p) iff
                  \[\begin{align}f(x)&=P(X=x)= \binom{x-1}{r-1}p^r(1-p)^{x-r}\quad for\  x=r,r+1,\ldots\end{align}\]
     - Theorem :: \[\sum_{x=r}^{x}\binom{x-1}{r-1}p^r(1-p)^{x-r}=1\]
     - Theorem :: if X~NB(r,p), then
                  a) $E(X) = \frac{r}{p}$
                  b) \[Var(x) = \frac{r(1-p)}{p^2}\]
     - Example :: Probability you hit target on any given shot is 0.42 (shooting range)
                  Assume shots are independent. Let X = shot on which you hit target for the 3rd time
                  X~NB(r=3,p=0.42)
                  a) Find probability target is hit for 3rd time on 5th shot.
                  \[\begin{align}P(x=5)&=\binom{x-1}{r-1}p^r(1-p)^{x-r}\\&=\binom{5-1}{3-1}0.42^3(1-.42)^{5-3}\\&=\binom{4}{2}0.42^3(0.58)^2 \\&=0.150\end{align}\]
                  b) On average how man shots needed for 3rd hit?
                  \[\begin{align}E(x)=\frac{r}{p}=\frac{3}{0.42}=7.14\end{align}\]
   - Geometric Distribution :: Negative binomial with r = 1
        NOTE: binomial distribution is analagous to sampling with replacement
   - Hypergeometric Distribution :: analagous to sampling without replacement
        Have N objects *M = success, N-M = failures
        Randomly select n objects without replacement
        if X = number of successes then X~HG(n,N,M)
        NOTE Randomly select n objects with replacement X=#success X~Bin(n,M/N)
   - X~HG(n,N,M) :: iff \[f(x)=P(X=x)=\frac{\binom{M}{n}\binom{N-M}{n-x}}{\binom{N}{n}}\]; x=0,1,...,n; $x\leq M$; $n-x\leq N-M$
   - Theorem :: If X~HG(n,N,M), then
                a) \[E(X)=n \frac{M}{N}\]
                b) \[Var(X)=n \frac{M}{N}(1- \frac{M}{N})(\frac{N-n}{N-1})\]; last term is finite population correction factor ($\leq 1$)
   - Example :: Bowl has 20 M&Ms, 4 of which are blue. RS 3 without replacement
                let X = # of Blue
                X~HG(n=3,N=20,M=4) NOTE: with replacement X~Bin(n=20,p=4/20)
                a) Find prob of 0 blue  *** DEBUG BELOW
                \[\begin{align}P(x=0)&=P(B_1^c\cap B_2^c\cap B_3^c)\\P(B_1^c)P(B_2^c|B_1^c)P(B_3^c|B_1^c\cap B_2^c)=0.491\\P(X=0)&=\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}=\frac{\binom{4}{0}\binom{20-4}{3-0}}{\binom{20}{3}}=0.491\end{align}\]
                b)Find probability of 2 Blue 
                \[P(X=2)&=P(B_1\cap B_2\cap B_3^c)+P(B_1\cap B_2^c\cap B_3)+P(B_1^c\cap B_2\cap B_3)\\P(X=2)&=\frac{\binom{M}{x}\binom{N-M}{n-x}}{\binom{N}{n}}\\&=\frac{\binom{4}{2}\binom{20-4}{3-2}}{\binom{20}{3}}}\]
                c) Expected # of B
                \[E(X)=n \frac{M}{N}=3 \frac{4}{20}=0.6\]
                NOTE: when N is large and n is small we can approx HG(n,N,M) with Bin(n,P=M/N)
   - Example :: 120 applicants, 80 are qualified. Randomly Select 5 applicants without replacement. X = # of qualified
                X~HG(n=5,N=120,M=80)
                \[P(X=2)=\frac{\binom{80}{2}\binom{120-80}{5-2}}{\binom{120}{5}}=0.164\]
                \[X\dot{~}Bin\left( n=5,p=\frac{80}{120}\right) \]
                \[P(X=2)=\binom{n}{x}p^x(1-p)^{n-x}=\binom{5}{2}\left(\frac{2}{3}\right)^2\left(1-\frac{2}{3}\right)^{5-2}\]
   - Poisson Distribution :: Suppose X is a random variable such that X = # of occurences in an interval or region where $\lambda$ = expected # of occurences in the interval or region
        ASSUME
        a) # of occurences in non-overlapping intervals or regions are independent
        b) probability of occurrence is proportional to $\lambda$ * length of interval
---------        c) TODO Add this assumption
        Then X~Pois($\lambda$)
   - X~Pois($\lambda$) :: iff
        \[f(x)=P(X=x)=\frac{e^{-\lambda}\lambda ^x}{x!}\]
        for x = 0,1,2,...$\infty$
        NOTE: \[\sum_{x-0}^{\infty}\frac{e^{-\lambda}\lambda^x}{x!}=e^{-\lambda}\sum_{x=0}^{\infty}\frac{\lambda^x}{x!}\]  (McLauren series) ;; check name of this
   - Theorem :: If X~Pois($\lambda$), then 
                a) E(X)=$\lambda$
                b)Var(X)=$\lambda$
   - Example :: Suppose particles hit a Geiger counter 5 times / sec on average, poisson assumptions hold
                Let X = # hits in one sec
                X~Pois($\lambda = 5$)
                a) Find probability of 0 hits in 1 sec
                  \[P(X=0)=\frac{e^{-5}5^0}{0!} = 0.0067\]
                b) \[P(X\geq 2)= P(x=2)+\ldots = 1-P(X<2) = 1 - P(X=0) - P(X=1) = 0.9596\]
                c) On average, how many hits do we expect in one sec $E(X) = \lambda = 5$
                d) Let Y = # hits in 1 min, find P(Y=0) 
                \[P(Y=0)= P(X_1=0,X_2=0,\ldots,X_{60}=0)=P(X_1)\ldotsP(X_{60})=.0067^{60}=5.148x10^{-131}\]
                \[Y~Pois(\lambda=5*60=300)\\P(Y=0)=\frac{e^{-\lambda}\lambda^y}{y!}\]
   - Example :: Avg defects/sq yard = 1.2
                a) Find probability of 1 defect in 1 sq yard
                X = # def in 1 sq. yard 
                \[P(X=1) = \frac{e^{-1.2}1.2^1}{1!}=.3614\]
                b) Find prob of 2 def. in 2 sq. yards
                X= # defects in 2 square yards 
                \[P(x=2)=p(x_1=1,x_2=1)+p(x_1=2,x_2=0)+p(x_1=0,x_2=2)=\frac{e^{-1.2}1.2^1}{1!}\frac{e^{-1.2}1.2^1}{1!}+\ldots\]
                X~Pois(2.4)
                \[P(X=2)=\frac{e^{-2.4}2.4^2}{2!}=.2613\]
               

** Chapter 4 :: Continuous Random Variable 
   - A continuous random variable can assum any value in an interval
   - Probability Density Function (PDF) :: Describes the probabalistic behavior of a cont. R.V. 
        The PDF satisfies
        a) $f(x)\leq0\forall x \in \real$  ** find big double r for \real
        b) $\int_{-\infty}^{\infty}f(x)dx = 1$  ** - to + infty
        NOTE: 
        a) $P(X=n)=0$
        b) $P(a<X<b)=P(a\leq X\leq b)$
        c) the pdf is proportional to the instantaneous prob, ie for small \epsilon
        $P(x-\frac{\epsilon}{2}<X<x+\frac{\epsilon}{2}\approx \epsilon f(x)$
   NOTE: ADD STUFF FROM NOTES HERE***  2/27/19
   - Example :: Calls arrive with $\lambda$ = 1/3 per minute
                a) find prob of 2 calls in nex min - P=.0398 (poisson)
                b) find prob wait for next call is < 10 minutes
                \[\begin{align}X<10)&=\int_{0}^{10}\frac{1}{3}e^{\frac{-1}{3}x}\\&=-e^{\frac{-10}{3}}+1\\&=.9654\end{align}\]
                \[\begin{align}P(X\geq 1)&=1-P(x<1)\\&=1-P(X=0)\\&=1-\frac{e^{\frac{-10}{3}}(\frac{10}{3})^0}{0!}=0.9643\end{align}\]
   c) Mean time b/w calls
   $E(X)=\frac{1}{\lambda}=\frac{1}{\frac{1}{3}}=3 min$
   d) find the 95th percentile of the waiting times \[\begin{align}0.95&=P(X\leq x)\\&=\int_{0}^{x}\frac{1}{3}e^{\frac{-1}{3}}dt\\&=-e^{\frac{-x}{3}}+1\end{align}\]
   - Theorem :: Lack of memory property
                If X~exp($\lambda$), then $P(X>t_1+t_2|X>t_1)=P(X>t_2)$
                e) $P(X>70|X>60)=P(X>10)$
   - Normal Distribution (Guassian) :: $X\textasciitilde N(\mu,\sigma^2)$ iff 
        \[f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2\]
   - Theorem :: \[\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx=1\]  (HARD TO PROVE)  
                E(X) = $\mu$
                Var(X)=$\sigma^2$
   - Theorem :: Empirical Rule
                If X~N($\mu , \sigma^2$), then 
                a) $P(\mu - \sigma<X<\mu + \sigma)\approx 0.68$
                b) $P(\mu - 2\sigma<X<\mu + 2\sigma)\approx 0.95$
                c) $P(\mu - 3\sigma<X<\mu + 3\sigma)\approx 0.997$
   - Example :: IQs, X have a normal distribution with mean $\mu = 100$ and var $\sigma^2=16^2$
                X~N($\mu=100,\sigma^2=16^2$)
                a) P(84<X<116)=0.68
                b) P(68<X<132)=0.95
                c) P(X>116)=0.16
                d) P(68<X<84)=0.135
                e) P(X>112)=??? (cant use empirical rule)
   NOTE: Standard normal distribution Z is a normal distribution with mean $\mu=0$ and variance $\sigma^2=1$
   Our standard normal table gives CDF of standard normal Z *** will use this a lot
   a) P(Z>0) = 0.5
   b) P(Z<0.32) = 0.6255 (from table)
   c) P(z>-.50)=1-0.3085 = 0.6915 (from table)
   d) find z such that 0.8508=P(Z<=z) => z=1.04
  - Theorem :: Standardization Theorem
               If X~N($\mu,\sigma^2$),then
               \[\frac{X-\mu}{\sigma}\] ~ N($\mu=0,\sigma^2$) ~ standard normal Z
  - Example :: IQs X~N($\mu=100,\sigma^2=16^2$)             
               a) \[P(X>112)=P(\frac{X-100}{16}>\frac{112-100}{16})=P(Z>0.75)\] <- .75 is called standard score or z-score
               b) \[\begin{align}P(84<X<116)&=P(\frac{84-100}{16}<\frac{x-100}{16}<\frac{116-100}{16})\\&=P(-1.00<Z<1.00)\\&=0.8413-.1587\\&=0.6826\end{align}\]

  NOTE: Look at 5.2.9 in book and answer - book solution is totally wrong
  - Example :: IQs X~N($\mu=100,\sigma^2=16^2$)
               Suppose Mensa only accepts people with an IQ in top 2%. How high must IQ be to get in?
               \[\begin{align}0.98=(P(X\leq x)&=P(\frac{X-100}{16}\leq \frac{x-100}{16})\\&=P(Z\leq \frac{x-100}{16})\\\frac{x-100}{16}&=2.05\\x&=132.8\end{align}\]
  - Example :: Weights of bags of apples 
               X~N($\mu, \sigma^2=0.2^2$)
               If 2.5% of the bags weigh less than 4.7 lbs, find $\mu$
               \[\begin{align}0.025 = P(X\leq4.7)&=P(\frac{x-\mu}{0.2}\leq \frac{4.7-\mu}{0.2})\\&=P(Z\leq \frac{4.7-\mu}{0.2})\\ \frac{4.7-\mu}{0.2}&=-1.96\\ \mu&=5.092 lbs\end{align}\]

** Chapter 5 :: Joint Distributions
   - Joint PMF :: suppose we have 2 sic R.V.s X&Y. Want to describe the joint probabilistic behavior. The joint pmf of X and Y is $f_{xy}(x,y)=P(X=x,Y=y)$
                  - example ::  $f_{xy}(x,y)=P(X=x,Y=y)$
                               $=\frac{x + y^2 +2}{25}$ x=0,1  y=0,1,2
                               |       |   |      | y    |       | fx(x) (marginal pmf) |
                               |       |   | 0    | 1    | 2     |                      |
                               | x     | 0 | 3/25 | 3/25 | 6/25  | 11/24                |
                               |       | 1 | 3/25 | 4/25 | 7/25  | 14/25                |
                               | fy(y) |   | 5/25 | 7/25 | 13/25 |                      |
                               a) $P(x=0,y\geq1) = 9/25$
                               b) $P(X=0)=f_{xy}(0,0)+f_{xy}(0,1)+f_{xy}(0,2)=11/25$
                               c) $P(X=Y)=fxy(0,0)+fxy(1,1)=6/25$
                               d) Marginal PMF of X
                                  \[\begin{align}f_x(x)&=P(X=x)\\&=\sum_{y}^{}f_{xy}\\&=\sum_{y=0}^{2}\frac{x + y^2 +2}{25}\\&=\frac{3x+11}{25}\qquad x=0,1\end{align}\]
                               e) Marginal PMF of Y 
                                  \[\begin{align}f_y(y)&=\sum_{x}^{}f_{xy}(x,y)\\&=\sum_{x=0}^{1}\frac{x+y^2+2}{25}\\&=\frac{2y^2+5}{25}\qquad y=0,1,2\end{align}\]
                               f) $P(Y\leq 1)=P(Y=0)+P(Y=1)=f_y(0)+f_y(1)=\frac{12}{25}$

                               g)n
                                 \[\begin{align}\mu_x&=E(x)\\&=\sum_{x}^{}xf_x(x)=\sum_{x=0}^{1}x\cdot \frac{3x+11}{25}=0\cdot \frac{3(0)+11}{25}+1\cdot \frac{3(1)+11}{25}\\&=\frac{14}{25}\end{align}\]
                               h) \[\sigma^2=Var(x)=E(x^2)-(E(x))^2= something\]
                               i) Conditional pmf of X given Y=y

                                  \[\begin{align}f_{x|Y=y}(x)&=P(X=x|Y=y)\\&=\frac{f_{xy}(x,y)}{f_y(y)}\\&=\frac{(x+y^2+2)/25}{(2y^2+5)/25}\\&=\frac{x+y^2+2}{2y^+5}\qquad x=0,1 (y=0,1,2)\end{align}\]
                                  \[\begin{align}f_{X|Y=0}(x)&=P(X=x|Y=0)\\&=\frac{x+2}{5}\qquad x=0,1\end{align}\]

                               j) \[\begin{align}\mu_{X|Y=y}&=E(X|Y=y)\\&=\sum_{x=0}^{1}x\cdot \frac{x+y^2+2}{2y^2+5}\&=\frac{ }{}\end{align}\]
                               
                               k) \[\begin{align}E(XY^2)&=\sum_{x}^{}\sum_{y}^{}xy^2\cdot f_{xy}(x,y)\\&=\sum_{x=0}^{1}\sum_{y=0}^{2}xy^2\cdot \frac{x+y^2+2}{25}\\&=\frac{32}{25}\end{align}\]
                               
  - Definition :: Independence: The disc RVs X and Y are indep iff
                  a) $f_{X|Y=y}(x)=f_x(x) \qquad\forall(x,y)$ in joint support
                  b) $f_{Y|X=x}(y)=f_y(y) \qquad\forall(x,y)$ in joint support
                  c) $f_{XY}(x,y)=f_x(x)f_y(y) \qquad\forall(x,y)$ in joint support
                  d) $P(X\in A, Y\in B)=P(X\in A)P(Y\in B) \qquad \forall$ sets A and B in the support of X and Y, respectively

  - Joint Probability Density Function (PDF) :: A joint PDF of 2 continuous RVs X and Y satisfies
       NOTE: add big R and little c
       a) $f_{xy}(x,y)\geq 0 \qquad \forall(x,y)\in \mathbb{R}^2$
       b) \[\int_{-\infty}^{\infty}\int_{\infty}^{\infty}f_{xy}(x,y)dxdy=1\]
       c) For any region $A \c  \mathbb{R}^2, P((X,Y)\in A)=\int_{A}^{}\int_{}^{}f_{xy}(x,y)dxdy$
   
  - Example :: Have 1 lb scrap copper and 2 lb of scrap tin, make alloy
               let X=amt of copper in allow and Y=amt of tin
               The join pdf of X and Y is  NOTE: change this to make the left squiggle piecewise thing
               \[f_{xy}(x,y)=\leftbracket \frac{3}{8}(x^2 + y)  0<x<1, 0<y<2 \\ 0 otherwise \]
               a) verify that joint pdf integrates to 1
               \[\begin{align}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{xy}(x,y)dxdy&=\int_{0}^{2}[\int_{0}^{1}\frac{3}{8}(x^2+y)dx]dy\\&=1\end{align}\]
               b) $P(x<\frac{1}{2},Y<1)
* Study Guide/Formula Sheet
  ADD TO THIS AND THEN JUST EXPORT OR COPY FOR TESTS

